{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-p PROFILE] [-m MODEL] [-t TRAIT]\n",
      "                             [-f FEATURESET] [-d DATABASE] [-b BATCHID]\n",
      "                             [--cal-method CAL_METHOD] [-s STARTCUTOFF]\n",
      "                             [-e ENDCUTOFF] [--timestamp TIMESTAMP]\n",
      "                             [--scoring SCORING] [--run RUN] [--pca PCA]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9018 --control=9016 --hb=9015 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"67ec183f-6439-4c45-bff4-445d8b4d0f8f\" --shell=9017 --transport=\"tcp\" --iopub=9019\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import contextlib\n",
    "from datetime import datetime\n",
    "import configs.config as config\n",
    "import configs.arguments_config as arguments_config\n",
    "from apr.functions import *\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedGroupKFold, PredefinedSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, auc, brier_score_loss, classification_report, confusion_matrix, f1_score, precision_score, recall_score, roc_auc_score, roc_curve, balanced_accuracy_score\n",
    "import pandas as pd\n",
    "import json\n",
    "import joblib\n",
    "from shutil import rmtree\n",
    "from tempfile import mkdtemp\n",
    "import random\n",
    "from random import randint\n",
    "import maestros.functions as mt\n",
    "from csv import writer\n",
    "from sklearn.utils import resample\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# parallel processing imports\n",
    "sys.modules['sklearn.externals.joblib'] = joblib\n",
    "from sklearn.externals.joblib import Parallel, parallel_backend\n",
    "from sklearn.externals.joblib import register_parallel_backend\n",
    "from sklearn.externals.joblib import delayed\n",
    "from sklearn.externals.joblib import cpu_count\n",
    "from ipyparallel import Client\n",
    "from ipyparallel.joblib import IPythonParallelBackend\n",
    "\n",
    "# --- IMPORT CONFIG ---\n",
    "\n",
    "db = config.db\n",
    "f = config.f\n",
    "t = config.t\n",
    "m = config.m\n",
    "sc = config.sc\n",
    "ec = config.ec\n",
    "label_feature_indexes = config.label_feature_indexes\n",
    "scoring = config.scoring\n",
    "scoring_metrics = config.scoring_metrics\n",
    "save_graphs = config.save_graphs\n",
    "save_model = config.save_model\n",
    "seed = config.seed\n",
    "n_searches = config.n_searches\n",
    "# pre_pars = config.pre_pars\n",
    "model_pars = config.model_pars\n",
    "clf_rfecv = config.clf_rfecv\n",
    "step_rfecv = config.step_rfecv\n",
    "calibration = config.calibration\n",
    "cal_method = config.cal_method\n",
    "n_bootstrap = config.n_bootstrap\n",
    "n_metadata_cols = config.n_metadata_cols\n",
    "gender = config.gender\n",
    "pca = config.pca\n",
    "\n",
    "# --- ARGUMENTS SETTINGS ---\n",
    "\n",
    "FILE_DIR = os.path.dirname(os.path.abspath(''))\n",
    "sys.path.append(FILE_DIR)\n",
    "\n",
    "# Default random value for batch\n",
    "random_id = randint(1, 10000)\n",
    "# Default value for run\n",
    "run = 0\n",
    "\n",
    "# Default current time for timestamp\n",
    "starttime = time.time()\n",
    "timestamp = datetime.now().strftime(\"%y%m%d_%H%M\")\n",
    "\n",
    "# prepare the logger\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-p\", \"--profile\", default=\"ipy_profile\",\n",
    "                    help=\"Name of IPython profile to use\")\n",
    "parser.add_argument(\"-m\", \"--model\", default=m,\n",
    "                    help=\"The model with which you want to train\")\n",
    "parser.add_argument(\"-t\", \"--trait\", default=t,\n",
    "                    help=\"The trait on which you want to train\")\n",
    "parser.add_argument(\"-f\", \"--featureset\", default=f,\n",
    "                    help=\"The feature set to use\")\n",
    "parser.add_argument(\"-d\", \"--database\", default=db, help=\"The database to train on\")\n",
    "parser.add_argument(\"-b\", \"--batchid\", default=str(random_id), help=\"The ID with which different searches can be grouped\")\n",
    "parser.add_argument(\"--cal-method\", default=cal_method,\n",
    "                    help=\"The calibration method. Can be 'sigmoid' or 'isotonic'\")\n",
    "parser.add_argument(\"-s\", \"--startcutoff\", default=str(sc),\n",
    "                    help=\"The minimum length of a sample\")\n",
    "parser.add_argument(\"-e\", \"--endcutoff\", default=str(ec),\n",
    "                    help=\"The maximum length of a sample\")\n",
    "parser.add_argument(\"--timestamp\", default=timestamp,\n",
    "                    help=\"Timestamp in format YYMMDD_hhmm, used for logging batches\")\n",
    "parser.add_argument(\"--scoring\", default=scoring,\n",
    "                    help=\"Scoring metric for training\")\n",
    "parser.add_argument(\"--run\", default=run,\n",
    "                    help=\"Scoring metric for training\")\n",
    "parser.add_argument(\"--pca\", default=str(pca),\n",
    "                    help=\"PCA to use\")\n",
    "\n",
    "# Overwrite config values with argument values\n",
    "args = parser.parse_args()\n",
    "profile = args.profile\n",
    "m = args.model\n",
    "t = args.trait\n",
    "f = args.featureset\n",
    "db = args.database\n",
    "b = args.batchid\n",
    "cal_method = args.cal_method\n",
    "sc = args.startcutoff\n",
    "ec = args.endcutoff\n",
    "sc = float(sc)\n",
    "ec = float(ec)\n",
    "timestamp = args.timestamp\n",
    "scoring = args.scoring\n",
    "run = args.run\n",
    "pca = str(args.pca)\n",
    "\n",
    "if pca == '99':\n",
    "    pca = PCA(0.99)\n",
    "elif pca == '95':\n",
    "    pca = PCA(0.95)\n",
    "else:\n",
    "    pca = 'passthrough'\n",
    "\n",
    "# Define index of labels and features after final database selection\n",
    "begin_col_labels = label_feature_indexes[db][0]\n",
    "begin_col_features = label_feature_indexes[db][1]\n",
    "median_labels_needed = label_feature_indexes[db][2]\n",
    "\n",
    "# Import custom arguments config\n",
    "trait_dict = arguments_config.trait_dict\n",
    "feat_dict = arguments_config.feat_dict\n",
    "database_dict = arguments_config.database_dict\n",
    "\n",
    "if t.capitalize() in trait_dict.keys():\n",
    "    t = trait_dict[t.capitalize()]\n",
    "\n",
    "if f.lower() in feat_dict.keys():\n",
    "    f = feat_dict[f.lower()]\n",
    "\n",
    "if db.lower() in database_dict.keys():\n",
    "    db = database_dict[db.lower()]\n",
    "\n",
    "# combine model and preprocessing parameters into one parameter grid\n",
    "# model_pars[m].update(pre_pars)\n",
    "param_grid = model_pars[m]\n",
    "\n",
    "# --- SETUP MULTIPROCESSING, LOGGING AND PATHS ---\n",
    "\n",
    "if calibration:\n",
    "    cal_str = cal_method\n",
    "else:\n",
    "    cal_str = \"no_cal\"\n",
    "\n",
    "# file_prefix = f\"{timestamp}-{b}-{db}-{scoring}-{sc}-{ec}-{m}-{f}-{t}-{cal_str}\".replace(\".\",\"_\")\n",
    "file_prefix = f\"{b}-{run}\"\n",
    "\n",
    "\n",
    "logfilename = os.path.join(\n",
    "    FILE_DIR, f'log/{timestamp}_{b}.log')\n",
    "logging.basicConfig(filename=logfilename,\n",
    "                    filemode='w',\n",
    "                    level=logging.DEBUG)\n",
    "logging.info(\"number of CPUs found: {0}\".format(cpu_count()))\n",
    "logging.info(\"args.profile: {0}\".format(profile))\n",
    "logging.info(\"Batch: %s\" % b)\n",
    "logging.info(\"FILE_DIR: %s\" % FILE_DIR)\n",
    "\n",
    "# check whether the batchid folder exists\n",
    "batch_path = f\"{FILE_DIR}/results/{timestamp}_{b}\"\n",
    "isExist = os.path.exists(batch_path)\n",
    "# if it doesn't exist, create new folder\n",
    "if not isExist:\n",
    "    os.makedirs(batch_path)\n",
    "    if save_model:\n",
    "        os.makedirs(f\"{batch_path}/best_estimators\")\n",
    "    os.makedirs(f\"{batch_path}/cv_results\")\n",
    "    os.makedirs(f\"{batch_path}/outputs\")\n",
    "    if save_graphs:\n",
    "        os.makedirs(f\"{batch_path}/graphs\")\n",
    "\n",
    "logging.info(\"Folders for batch checked or made\")\n",
    "\n",
    "output_path = f\"{batch_path}/outputs/{file_prefix}-output.txt\"\n",
    "\n",
    "ps = PrintSaver(output_path)\n",
    "\n",
    "# prepare the engines\n",
    "c = Client(profile=profile)\n",
    "# The following command will make sure that each engine is running in\n",
    "# the right working directory to access the custom function(s).\n",
    "c[:].map(os.chdir, [FILE_DIR]*len(c))\n",
    "\n",
    "logging.info(\"c.ids :{0}\".format(str(c.ids)))\n",
    "bview = c.load_balanced_view()\n",
    "register_parallel_backend('ipyparallel',\n",
    "                          lambda: IPythonParallelBackend(view=bview))\n",
    "\n",
    "ps.print_save(f\"\\nSetup:\")\n",
    "ps.print_save(f\"b: {b}, db: {db}, sc: {sc}, ec: {ec}, f: {f}, m: {m}, t: {t}, scoring: {scoring}, cal: {cal_str}\")\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "\n",
    "# load dataset\n",
    "full_df = pd.read_pickle(f\"{FILE_DIR}/data/{db}\")\n",
    "\n",
    "if gender == 'male':\n",
    "    full_df = full_df[full_df['Gender'] == 1]\n",
    "elif gender == 'female':\n",
    "    full_df = full_df[full_df['Gender'] == 0]\n",
    "else:\n",
    "    gender = 'both'\n",
    "\n",
    "if median_labels_needed:\n",
    "    'Calculating median labels...'\n",
    "    full_df = calculate_median_labels(full_df, begin_col_labels, begin_col_features)\n",
    "\n",
    "# adjust time cutoff\n",
    "if sc != 0 and 'Length' in full_df.columns:\n",
    "    full_df = full_df[(full_df['Length'] >= sc)]\n",
    "if ec != 0 and 'Length' in full_df.columns:\n",
    "    full_df = full_df[(full_df['Length'] <= ec)]\n",
    "\n",
    "# define X and y\n",
    "X = full_df.iloc[:, begin_col_features:]\n",
    "y = full_df.iloc[:,begin_col_labels - n_metadata_cols:begin_col_features]\n",
    "\n",
    "# define feature_names, label_names and groups\n",
    "feature_names = list(X.columns.values)\n",
    "label_names = list(y.columns.values)\n",
    "groups = full_df['Group'].to_numpy()\n",
    "\n",
    "\n",
    "# convert to numpy\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "ps.print_save(f\"\\nTotal number of samples: {len(y[:,0])}\")\n",
    "\n",
    "# --- SPLIT TRAIN, VAL, TEST ---\n",
    "\n",
    "# Number of labels\n",
    "y_n_cols = y.shape[n_metadata_cols]\n",
    "\n",
    "if y_n_cols > 1:\n",
    "    # -- Multi label split --\n",
    "    # Choose the label to train on\n",
    "    t_idx = label_names.index(t)\n",
    "\n",
    "    # If with calibration, create validation set for calibration\n",
    "    if calibration:\n",
    "        # Initial train_val-test split (80% train_val, 20% test)\n",
    "        X_train_val, X_test, y_train_vals, y_tests, train_val_indices, test_indices = mt.multilabel_stratified_group_split(X, y, groups, test_size=0.2, random_state=seed)\n",
    "\n",
    "        groups_train_val = groups[train_val_indices]\n",
    "        groups_test = groups[test_indices]\n",
    "\n",
    "        \n",
    "        # Split train_val into train and val for calibration (of whole dataset: 60% train, 20% validation)\n",
    "        X_train, X_val, y_trains, y_vals, train_indices, val_indices = mt.multilabel_stratified_group_split(X_train_val, y_train_vals, groups_train_val, test_size=0.20, random_state=seed)\n",
    "\n",
    "        groups_train = groups_train_val[train_indices]\n",
    "        groups_val = groups_train_val[val_indices]\n",
    "\n",
    "        ps.print_save(mt.stratification_report(y, y_trains, y_tests, y_val=y_vals, labels=label_names))\n",
    "\n",
    "        y_val = y_vals[:,t_idx]\n",
    "    else:\n",
    "        # Initial train-test split (80% train, 20% test)\n",
    "        X_train, X_test, y_trains, y_tests, train_indices, test_indices = mt.multilabel_stratified_group_split(X, y, groups, test_size=0.2, random_state=seed)\n",
    "\n",
    "        groups_train = groups[train_indices]\n",
    "        groups_test = groups[test_indices]\n",
    "\n",
    "        ps.print_save(mt.stratification_report(y, y_trains, y_tests, labels=label_names))\n",
    "\n",
    "    \n",
    "    y_train = y_trains[:,t_idx]\n",
    "    y_test = y_tests[:,t_idx]\n",
    "else:\n",
    "    # -- Single label split --\n",
    "\n",
    "    # Convert single column y to 1D array\n",
    "    y = y.ravel()\n",
    "\n",
    "    if calibration:\n",
    "        # Initial train_val-test split (80% train_val, 20% test)\n",
    "        cv_split = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        train_val_idx, test_idx = next(cv_split.split(X, y, groups))\n",
    "        X_train_val, X_test, y_train_val, y_test = X[train_val_idx], X[test_idx], y[train_val_idx], y[test_idx]\n",
    "        groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "        # Split train_val into train and val for calibration (of whole dataset: 60% train, 20% validation)\n",
    "        cv_train_val = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=seed)\n",
    "        train_idx, val_idx = next(cv_train_val.split(X_train_val, y_train_val, groups_train_val))\n",
    "        X_train, X_val, y_train, y_val = X_train_val[train_idx], X_train_val[val_idx], y_train_val[train_idx], y_train_val[val_idx]\n",
    "        groups_train, groups_val = groups_train_val[train_idx], groups_train_val[val_idx]\n",
    "    else:\n",
    "        # Initial train-test split (80% train, 20% test)\n",
    "        cv_split = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        train_idx, test_idx = next(cv_split.split(X, y, groups))\n",
    "        X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]\n",
    "        groups_train, groups_test = groups[train_idx], groups[test_idx]\n",
    "\n",
    "# --- TRAINING ---\n",
    "\n",
    "# Variables to keep track of the best model and score\n",
    "reports = [None] * n_searches\n",
    "\n",
    "# Manually iterate over the outer StratifiedGroupKFold for GridSearchCV\n",
    "cv_gs = StratifiedGroupKFold(n_splits=n_searches, shuffle=True, random_state=seed)\n",
    "\n",
    "ps.print_save(f\"Training {t}\")\n",
    "# -- cross-validation of the GridSearch --\n",
    "for idx, (train_idx_gs, test_idx_gs) in enumerate(StratifiedGroupKFold(n_splits=n_searches, shuffle=True, random_state=seed).split(X_train, y_train, groups_train)):\n",
    "    print(f\"Fold {idx+1}\")\n",
    "    X_train_gs, y_train_gs, X_test_gs, y_test_gs = X_train[train_idx_gs], y_train[train_idx_gs], X_train[test_idx_gs], y_train[test_idx_gs]\n",
    "    groups_train_gs = groups_train[train_idx_gs]\n",
    "    groups_test_gs =  groups_train[test_idx_gs]\n",
    "\n",
    "    # Create a StratifiedGroupKFold object for the RFECV\n",
    "    cv_rfecv = BalancedStratifiedGroupKFold(n_splits=5, max_attempts=1000, print_saver=ps, outer_fold=(idx+1))\n",
    "\n",
    "    test_fold = np.zeros(len(y_train))\n",
    "    test_fold[train_idx_gs] = -1\n",
    "\n",
    "    cv_fold = PredefinedSplit(test_fold=test_fold)\n",
    "\n",
    "    cache = mkdtemp()\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', pca),\n",
    "        ('feat_sel', RFECV(clf_rfecv, cv=list(cv_rfecv.split(X_train_gs, y_train_gs, groups_train_gs)), scoring=scoring, step=step_rfecv)),\n",
    "        ('clf', SVC(kernel='rbf'))\n",
    "    ],memory=cache)\n",
    "\n",
    "    # Perform a Grid Search for the current fold\n",
    "    gs = GridSearchCV(pipe, param_grid, scoring=scoring_metrics, n_jobs=len(c), cv=cv_fold, error_score='raise', verbose=1, refit=scoring)\n",
    "\n",
    "    # run with multiprocessing\n",
    "    with parallel_backend('ipyparallel'):\n",
    "        gs.fit(X_train, y_train)\n",
    "\n",
    "    reports[idx] = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "    rmtree(cache)\n",
    "\n",
    "scoring_names = scoring_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the cv_results_ of the different folds\n",
    "cv_results_ = merge_cv_results(reports, scoring_metrics=scoring_names, main_metric=scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the best parameters\n",
    "best_params_ = cv_results_.loc[cv_results_.index[0],'params']\n",
    "\n",
    "for x in scoring_names:\n",
    "    if x != scoring:\n",
    "        second_scoring = x\n",
    "\n",
    "cv_combined_test_score = cv_results_.loc[cv_results_.index[0],f'combined_test_{scoring}']\n",
    "cv_mean_test_main = cv_results_.loc[cv_results_.index[0],f'mean_test_{scoring}']\n",
    "cv_std_test_main = cv_results_.loc[cv_results_.index[0],f'std_test_{scoring}']\n",
    "cv_mean_test_second = cv_results_.loc[cv_results_.index[0],f'mean_test_{second_scoring}']\n",
    "cv_std_test_second = cv_results_.loc[cv_results_.index[0],f'std_test_{second_scoring}']\n",
    "cv_rank_test_score = cv_results_.loc[cv_results_.index[0],f'rank_test_score']\n",
    "\n",
    "\n",
    "# create a StratifiedGroupKFold object for the best estimator RFECV\n",
    "cv_rfecv_best = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "# Create the best estimator pipeline\n",
    "best_estimator_ = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', pca),\n",
    "    ('feat_sel', RFECV(clf_rfecv, cv=list(cv_rfecv_best.split(X_train_gs, y_train_gs, groups_train_gs)), scoring=scoring)),\n",
    "    ('clf', 'passthrough')\n",
    "])\n",
    "\n",
    "# Set the best parameters\n",
    "best_estimator_.set_params(**best_params_)\n",
    "\n",
    "# Fit the best estimator on the whole training set\n",
    "best_estimator_.fit(X_train, y_train)\n",
    "\n",
    "final_estimator_ = best_estimator_\n",
    "\n",
    "if calibration:\n",
    "    # Calibrate the model using the validation set\n",
    "    calibrated_estimator_ = CalibratedClassifierCV(best_estimator_, cv='prefit', method=cal_method)\n",
    "    calibrated_estimator_.fit(X_val, y_val)\n",
    "    final_estimator_ = calibrated_estimator_\n",
    "\n",
    "# --- EVALUATION ---\n",
    "\n",
    "ps.print_save(f\"\\n{str(best_params_)}\")\n",
    "\n",
    "# Evaluation test set using bootstrapping\n",
    "\n",
    "# Initialize arrays to store bootstrapped metrics\n",
    "boot_auc_rocs = []\n",
    "boot_bal_accs = []\n",
    "boot_f1_scores = []\n",
    "boot_precisions = []\n",
    "boot_recalls = []\n",
    "\n",
    "# Bootstrap loop\n",
    "print(\"\\nBootstrapping...\")\n",
    "for bs_idx in range(n_bootstrap):\n",
    "    if (bs_idx+1) % (n_bootstrap / 10) == 0:\n",
    "        print(f\"Bootstrap {bs_idx+1}/{n_bootstrap}\")\n",
    "    # Resample the test set with replacement\n",
    "    X_resampled, y_resampled = resample(X_test, y_test, replace=True, random_state=bs_idx, n_samples=len(y_test))\n",
    "\n",
    "    # Predict probabilities and labels for the resampled test set\n",
    "    y_probs = final_estimator_.predict_proba(X_resampled)[:, 1]\n",
    "    y_preds = final_estimator_.predict(X_resampled)\n",
    "\n",
    "    # Compute the evaluation metrics\n",
    "    fpr, tpr, _ = roc_curve(y_resampled, y_probs)\n",
    "    boot_auc_rocs.append(roc_auc_score(y_resampled, y_probs))\n",
    "    boot_bal_accs.append(balanced_accuracy_score(y_resampled, y_preds))\n",
    "    boot_f1_scores.append(f1_score(y_resampled, y_preds))\n",
    "    boot_precisions.append(precision_score(y_resampled, y_preds))\n",
    "    boot_recalls.append(recall_score(y_resampled, y_preds))\n",
    "\n",
    "print(\"Bootstrapping done!\")\n",
    "\n",
    "# Calculate the mean and standard deviation for each metric\n",
    "auc_roc_mean = round(np.mean(boot_auc_rocs), 3)\n",
    "auc_roc_std = round(np.std(boot_auc_rocs), 3)\n",
    "bal_acc_mean = round(np.mean(boot_bal_accs), 3)\n",
    "bal_acc_std = round(np.std(boot_bal_accs), 3)\n",
    "f1_mean = round(np.mean(boot_f1_scores), 3)\n",
    "f1_std = round(np.std(boot_f1_scores), 3)\n",
    "precision_mean = round(np.mean(boot_precisions), 3)\n",
    "precision_std = round(np.std(boot_precisions), 3)\n",
    "recall_mean = round(np.mean(boot_recalls), 3)\n",
    "recall_std = round(np.std(boot_recalls), 3)\n",
    "\n",
    "# Calculate the 95% confidence intervals for each metric\n",
    "alpha = 0.95\n",
    "auc_roc_ci = np.percentile(boot_auc_rocs, [(1 - alpha) / 2 * 100, (1 + alpha) / 2 * 100])\n",
    "bal_acc_ci = np.percentile(boot_bal_accs, [(1 - alpha) / 2 * 100, (1 + alpha) / 2 * 100])\n",
    "f1_ci = np.percentile(boot_f1_scores, [(1 - alpha) / 2 * 100, (1 + alpha) / 2 * 100])\n",
    "precision_ci = np.percentile(boot_precisions, [(1 - alpha) / 2 * 100, (1 + alpha) / 2 * 100])\n",
    "recall_ci = np.percentile(boot_recalls, [(1 - alpha) / 2 * 100, (1 + alpha) / 2 * 100])\n",
    "\n",
    "# Predict probabilities for the original test set\n",
    "y_probs = final_estimator_.predict_proba(X_test)[:, 1]\n",
    "y_preds = final_estimator_.predict(X_test)\n",
    "\n",
    "# Scoring metrics of original test set\n",
    "auc_roc = roc_auc_score(y_test, y_probs)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_preds)\n",
    "f1 = f1_score(y_test, y_preds)\n",
    "precision = precision_score(y_test, y_preds)\n",
    "recall = recall_score(y_test, y_preds)\n",
    "\n",
    "# Classification report of original tes tset\n",
    "ps.print_save(f\"\\nClassification report original test set:\\n{classification_report(y_test, y_preds)}\")\n",
    "\n",
    "# Confusion matrix of original test set\n",
    "ps.print_save(f\"Confusion matrix original test set:\\n{confusion_matrix(y_test, y_preds)}\")\n",
    "\n",
    "ps.print_save(\"\\nTraining set CV best model:\")\n",
    "ps.print_save(f\"Best combined score: {round(cv_combined_test_score, 3)}\")\n",
    "ps.print_save(f\"Best mean score: {round(cv_mean_test_main, 3)}\")\n",
    "ps.print_save(f\"Best std score: {round(cv_std_test_main, 3)}\")\n",
    "ps.print_save(f\"Rank of mean score: {cv_rank_test_score}\")\n",
    "\n",
    "# Print bootstrapped results\n",
    "ps.print_save(\"\\nEvaluation bootstrap results (mean, SD, lower CI, higher CI):\")\n",
    "\n",
    "ps.print_save(f\"AUC-ROC score: ({auc_roc_mean:.3f}, {auc_roc_std:.3f}, {auc_roc_ci[0]:.3f}, {auc_roc_ci[1]:.3f})\")\n",
    "ps.print_save(f\"Balanced accuracy: ({bal_acc_mean:.3f}, {bal_acc_std:.3f}, {bal_acc_ci[0]:.3f}, {bal_acc_ci[1]:.3f})\")\n",
    "ps.print_save(f\"F1 score: ({f1_mean:.3f}, {f1_std:.3f}, {f1_ci[0]:.3f}, {f1_ci[1]:.3f})\")\n",
    "ps.print_save(f\"Precision: ({precision_mean:.3f}, {precision_std:.3f}, {precision_ci[0]:.3f}, {precision_ci[1]:.3f})\")\n",
    "ps.print_save(f\"Recall: ({recall_mean:.3f}, {recall_std:.3f}, {recall_ci[0]:.3f}, {recall_ci[1]:.3f})\")\n",
    "\n",
    "db_name = db.split(\"-\")[0]\n",
    "# create the string to add to the main results file\n",
    "main_results_string = [timestamp, b, run, db_name, sc, ec, f, m, t]\n",
    "\n",
    "for i, x in enumerate(best_estimator_):\n",
    "    if i == 1:\n",
    "        if str(x) != 'passthrough':\n",
    "            x = f\"{str(x).split('=')[1][:-1]}, {x.n_components_}\"\n",
    "    if i == 2:\n",
    "        x = x.n_features_\n",
    "    main_results_string.append(str(x).replace('\\n', ''))\n",
    "\n",
    "main_results_string.extend([\n",
    "    cal_str, scoring,\n",
    "    round(cv_combined_test_score, 3), cv_rank_test_score, round(cv_mean_test_main, 3), round(cv_std_test_main, 3), round(cv_mean_test_second, 3), round(cv_std_test_second, 3),\n",
    "    round(auc_roc, 3), round(auc_roc_mean, 3), round(auc_roc_std, 3),\n",
    "    round(auc_roc_ci[0], 3), round(auc_roc_ci[1], 3),\n",
    "    round(bal_acc, 3), round(bal_acc_mean, 3), round(bal_acc_std, 3),\n",
    "    round(bal_acc_ci[0], 3), round(bal_acc_ci[1], 3),\n",
    "    round(f1, 3), round(f1_mean, 3), round(f1_std, 3),\n",
    "    round(f1_ci[0], 3), round(f1_ci[1], 3),\n",
    "    round(precision, 3), round(precision_mean, 3), round(precision_std, 3),\n",
    "    round(precision_ci[0], 3), round(precision_ci[1], 3),\n",
    "    round(recall, 3), round(recall_mean, 3), round(recall_std, 3),\n",
    "    round(recall_ci[0], 3), round(recall_ci[1], 3),\n",
    "])\n",
    "\n",
    "\n",
    "# --- SAVE RESULTS ---\n",
    "\n",
    "# Save cv_results_ to file\n",
    "cv_results_.to_csv(f'{batch_path}/cv_results/{file_prefix}-best_result.csv')\n",
    "\n",
    "if save_model:\n",
    "    joblib.dump(best_estimator_, f\"{batch_path}/best_estimators/{file_prefix}-best_estimator.joblib\")\n",
    "\n",
    "\n",
    "# write string to the main results file\n",
    "with open(f\"{FILE_DIR}/results/main_results.csv\", 'a') as fw:\n",
    "    w = writer(fw, delimiter=';')\n",
    "    w.writerow(main_results_string)\n",
    "    fw.close()\n",
    "\n",
    "print(\"\\nFiles saved.\")\n",
    "\n",
    "# Print total duration of execution\n",
    "ps.print_save(f\"\\nTotal time: {round((time.time() - starttime),1)}s\")\n",
    "\n",
    "if save_graphs:\n",
    "    roc_name = f\"{batch_path}/graphs/roc_curve-{file_prefix}.png\"\n",
    "    # Calculate the ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_probs)\n",
    "\n",
    "    # Plot the ROC curve\n",
    "    plt.plot(fpr, tpr, label=f'AUC-ROC: {auc_roc:.3f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Random classifier')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) curve')\n",
    "    plt.savefig(os.path.join(roc_name), format='png')  # Save the plot to a file\n",
    "    plt.clf()  # Clear the current plot\n",
    "\n",
    "    # Calibration curve\n",
    "    cal_name = f\"{batch_path}/graphs/cal_curve-{file_prefix}.png\"\n",
    "    true_proportions, predicted_proportions = calibration_curve(y_test, y_probs, n_bins=10)\n",
    "\n",
    "    # Plot the calibration curve\n",
    "    plt.plot(predicted_proportions, true_proportions, marker='o', label='Calibrated model')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "    plt.xlabel('Predicted probability')\n",
    "    plt.ylabel('True probability')\n",
    "    plt.title('Calibration curve')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(cal_name), format='png')  # Save the plot to a file\n",
    "    plt.clf()  # Clear the current plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
