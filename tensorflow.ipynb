{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TF 2.X and make sure we're running eager.\n",
    "import tensorflow as tf\n",
    "assert tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 11:42:52.851624: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 11:42:52.853015: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Load the module and run inference.\n",
    "module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "y, sr = librosa.load('/media/emile/disk/Chatbot_10_15/USER_01_CB2_00_00.wav', sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04076386, 0.07565035, 0.09392864, ..., 0.        , 0.        ,\n",
       "       0.        ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = module(samples=y, sample_rate=16000)['embedding']\n",
    "# `emb` is a [time, feature_dim] Tensor.\n",
    "emb.shape.assert_is_compatible_with([None, 2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2038</th>\n",
       "      <th>2039</th>\n",
       "      <th>2040</th>\n",
       "      <th>2041</th>\n",
       "      <th>2042</th>\n",
       "      <th>2043</th>\n",
       "      <th>2044</th>\n",
       "      <th>2045</th>\n",
       "      <th>2046</th>\n",
       "      <th>2047</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.337506</td>\n",
       "      <td>-0.040675</td>\n",
       "      <td>0.080816</td>\n",
       "      <td>0.088913</td>\n",
       "      <td>0.147161</td>\n",
       "      <td>0.321658</td>\n",
       "      <td>0.258711</td>\n",
       "      <td>0.262652</td>\n",
       "      <td>0.050555</td>\n",
       "      <td>0.434577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106480</td>\n",
       "      <td>0.184365</td>\n",
       "      <td>0.355496</td>\n",
       "      <td>0.100770</td>\n",
       "      <td>0.168504</td>\n",
       "      <td>0.167772</td>\n",
       "      <td>0.072285</td>\n",
       "      <td>0.403610</td>\n",
       "      <td>0.178494</td>\n",
       "      <td>0.528386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.122066</td>\n",
       "      <td>0.174515</td>\n",
       "      <td>0.323350</td>\n",
       "      <td>0.372318</td>\n",
       "      <td>0.106595</td>\n",
       "      <td>0.511789</td>\n",
       "      <td>0.276275</td>\n",
       "      <td>0.705499</td>\n",
       "      <td>0.256095</td>\n",
       "      <td>0.225592</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196828</td>\n",
       "      <td>0.587497</td>\n",
       "      <td>0.108741</td>\n",
       "      <td>0.274103</td>\n",
       "      <td>0.132199</td>\n",
       "      <td>0.165451</td>\n",
       "      <td>0.232621</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.169190</td>\n",
       "      <td>0.138179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102713</td>\n",
       "      <td>0.282442</td>\n",
       "      <td>0.416054</td>\n",
       "      <td>0.706489</td>\n",
       "      <td>0.224125</td>\n",
       "      <td>0.040456</td>\n",
       "      <td>0.179622</td>\n",
       "      <td>0.166533</td>\n",
       "      <td>0.048880</td>\n",
       "      <td>0.277912</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128521</td>\n",
       "      <td>0.492704</td>\n",
       "      <td>0.276728</td>\n",
       "      <td>0.088314</td>\n",
       "      <td>0.475909</td>\n",
       "      <td>0.065890</td>\n",
       "      <td>0.408152</td>\n",
       "      <td>0.698953</td>\n",
       "      <td>0.366564</td>\n",
       "      <td>0.065407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.038816</td>\n",
       "      <td>-0.045828</td>\n",
       "      <td>0.168345</td>\n",
       "      <td>0.485001</td>\n",
       "      <td>0.390206</td>\n",
       "      <td>-0.079251</td>\n",
       "      <td>0.245818</td>\n",
       "      <td>0.226432</td>\n",
       "      <td>0.383727</td>\n",
       "      <td>0.344295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.424098</td>\n",
       "      <td>0.288179</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>0.355201</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.266953</td>\n",
       "      <td>0.307074</td>\n",
       "      <td>0.524771</td>\n",
       "      <td>0.199257</td>\n",
       "      <td>-0.051795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.159759</td>\n",
       "      <td>0.087066</td>\n",
       "      <td>0.219032</td>\n",
       "      <td>0.126468</td>\n",
       "      <td>0.451057</td>\n",
       "      <td>0.175705</td>\n",
       "      <td>0.082451</td>\n",
       "      <td>0.141692</td>\n",
       "      <td>0.411304</td>\n",
       "      <td>0.419022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.296449</td>\n",
       "      <td>0.041784</td>\n",
       "      <td>0.246841</td>\n",
       "      <td>0.166529</td>\n",
       "      <td>0.272051</td>\n",
       "      <td>0.182839</td>\n",
       "      <td>0.410285</td>\n",
       "      <td>0.312769</td>\n",
       "      <td>0.257206</td>\n",
       "      <td>0.484821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>0.164002</td>\n",
       "      <td>0.125804</td>\n",
       "      <td>0.147179</td>\n",
       "      <td>-0.025500</td>\n",
       "      <td>-0.041062</td>\n",
       "      <td>0.351944</td>\n",
       "      <td>0.658185</td>\n",
       "      <td>0.287136</td>\n",
       "      <td>0.164369</td>\n",
       "      <td>0.097967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.157654</td>\n",
       "      <td>0.215302</td>\n",
       "      <td>0.398865</td>\n",
       "      <td>-0.214418</td>\n",
       "      <td>-0.119118</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>0.295848</td>\n",
       "      <td>0.418526</td>\n",
       "      <td>0.312677</td>\n",
       "      <td>0.374377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.144374</td>\n",
       "      <td>0.296652</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.301183</td>\n",
       "      <td>0.267612</td>\n",
       "      <td>0.369479</td>\n",
       "      <td>0.241292</td>\n",
       "      <td>0.082927</td>\n",
       "      <td>0.078578</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232222</td>\n",
       "      <td>0.111789</td>\n",
       "      <td>0.183152</td>\n",
       "      <td>0.086149</td>\n",
       "      <td>0.483691</td>\n",
       "      <td>0.045824</td>\n",
       "      <td>0.372664</td>\n",
       "      <td>0.347135</td>\n",
       "      <td>0.391633</td>\n",
       "      <td>0.158330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.001786</td>\n",
       "      <td>0.351002</td>\n",
       "      <td>0.223638</td>\n",
       "      <td>0.333136</td>\n",
       "      <td>0.488250</td>\n",
       "      <td>0.256204</td>\n",
       "      <td>0.013433</td>\n",
       "      <td>0.326276</td>\n",
       "      <td>0.043618</td>\n",
       "      <td>0.375454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324294</td>\n",
       "      <td>0.077265</td>\n",
       "      <td>0.199864</td>\n",
       "      <td>0.295231</td>\n",
       "      <td>0.403349</td>\n",
       "      <td>0.245053</td>\n",
       "      <td>0.311306</td>\n",
       "      <td>0.289344</td>\n",
       "      <td>0.191157</td>\n",
       "      <td>0.228498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.138538</td>\n",
       "      <td>0.097028</td>\n",
       "      <td>0.389385</td>\n",
       "      <td>0.470858</td>\n",
       "      <td>-0.134868</td>\n",
       "      <td>0.185367</td>\n",
       "      <td>0.226538</td>\n",
       "      <td>0.027415</td>\n",
       "      <td>0.277127</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260614</td>\n",
       "      <td>0.196878</td>\n",
       "      <td>0.309701</td>\n",
       "      <td>0.424242</td>\n",
       "      <td>0.313489</td>\n",
       "      <td>0.192337</td>\n",
       "      <td>0.183395</td>\n",
       "      <td>0.068969</td>\n",
       "      <td>0.114205</td>\n",
       "      <td>0.246835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.117401</td>\n",
       "      <td>0.129924</td>\n",
       "      <td>0.037877</td>\n",
       "      <td>0.212045</td>\n",
       "      <td>0.278560</td>\n",
       "      <td>-0.045290</td>\n",
       "      <td>0.052637</td>\n",
       "      <td>0.224551</td>\n",
       "      <td>-0.047306</td>\n",
       "      <td>0.094918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.276126</td>\n",
       "      <td>0.156459</td>\n",
       "      <td>0.295758</td>\n",
       "      <td>0.268446</td>\n",
       "      <td>0.035386</td>\n",
       "      <td>0.045996</td>\n",
       "      <td>0.205074</td>\n",
       "      <td>0.033418</td>\n",
       "      <td>0.245389</td>\n",
       "      <td>0.287909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows Ã— 2048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "0   0.337506 -0.040675  0.080816  0.088913  0.147161  0.321658  0.258711   \n",
       "1   0.122066  0.174515  0.323350  0.372318  0.106595  0.511789  0.276275   \n",
       "2   0.102713  0.282442  0.416054  0.706489  0.224125  0.040456  0.179622   \n",
       "3   0.038816 -0.045828  0.168345  0.485001  0.390206 -0.079251  0.245818   \n",
       "4   0.159759  0.087066  0.219032  0.126468  0.451057  0.175705  0.082451   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "74  0.164002  0.125804  0.147179 -0.025500 -0.041062  0.351944  0.658185   \n",
       "75  0.144374  0.296652  0.199200  0.301183  0.267612  0.369479  0.241292   \n",
       "76  0.001786  0.351002  0.223638  0.333136  0.488250  0.256204  0.013433   \n",
       "77  0.090239  0.138538  0.097028  0.389385  0.470858 -0.134868  0.185367   \n",
       "78  0.117401  0.129924  0.037877  0.212045  0.278560 -0.045290  0.052637   \n",
       "\n",
       "        7         8         9     ...      2038      2039      2040      2041  \\\n",
       "0   0.262652  0.050555  0.434577  ...  0.106480  0.184365  0.355496  0.100770   \n",
       "1   0.705499  0.256095  0.225592  ... -0.196828  0.587497  0.108741  0.274103   \n",
       "2   0.166533  0.048880  0.277912  ...  0.128521  0.492704  0.276728  0.088314   \n",
       "3   0.226432  0.383727  0.344295  ...  0.424098  0.288179  0.293488  0.355201   \n",
       "4   0.141692  0.411304  0.419022  ...  0.296449  0.041784  0.246841  0.166529   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "74  0.287136  0.164369  0.097967  ...  0.157654  0.215302  0.398865 -0.214418   \n",
       "75  0.082927  0.078578  0.000010  ...  0.232222  0.111789  0.183152  0.086149   \n",
       "76  0.326276  0.043618  0.375454  ...  0.324294  0.077265  0.199864  0.295231   \n",
       "77  0.226538  0.027415  0.277127  ...  0.260614  0.196878  0.309701  0.424242   \n",
       "78  0.224551 -0.047306  0.094918  ...  0.276126  0.156459  0.295758  0.268446   \n",
       "\n",
       "        2042      2043      2044      2045      2046      2047  \n",
       "0   0.168504  0.167772  0.072285  0.403610  0.178494  0.528386  \n",
       "1   0.132199  0.165451  0.232621  0.509083  0.169190  0.138179  \n",
       "2   0.475909  0.065890  0.408152  0.698953  0.366564  0.065407  \n",
       "3   0.269071  0.266953  0.307074  0.524771  0.199257 -0.051795  \n",
       "4   0.272051  0.182839  0.410285  0.312769  0.257206  0.484821  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "74 -0.119118 -0.001416  0.295848  0.418526  0.312677  0.374377  \n",
       "75  0.483691  0.045824  0.372664  0.347135  0.391633  0.158330  \n",
       "76  0.403349  0.245053  0.311306  0.289344  0.191157  0.228498  \n",
       "77  0.313489  0.192337  0.183395  0.068969  0.114205  0.246835  \n",
       "78  0.035386  0.045996  0.205074  0.033418  0.245389  0.287909  \n",
       "\n",
       "[79 rows x 2048 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(emb)\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(emb.numpy())\n",
    "# emb.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 64000), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_samples = tf.zeros([3, 64000])\n",
    "audio_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.74227766e-08, -4.94537354e-02, -9.87865329e-02, -1.47877350e-01,\n",
       "       -1.96606517e-01, -2.44854316e-01, -2.92503148e-01, -3.39435965e-01,\n",
       "       -3.85538340e-01, -4.30697232e-01, -4.74801928e-01, -5.17744958e-01,\n",
       "       -5.59420705e-01, -5.99727690e-01, -6.38566852e-01, -6.75843477e-01,\n",
       "       -7.11466193e-01, -7.45347679e-01, -7.77405381e-01, -8.07560444e-01,\n",
       "       -8.35739434e-01, -8.61873090e-01, -8.85897756e-01, -9.07754481e-01,\n",
       "       -9.27389622e-01, -9.44755375e-01, -9.59809184e-01, -9.72514093e-01,\n",
       "       -9.82839167e-01, -9.90759015e-01, -9.96254325e-01, -9.99311686e-01,\n",
       "       -9.99923527e-01, -9.98088360e-01, -9.93810713e-01, -9.87101078e-01,\n",
       "       -9.77975845e-01, -9.66457307e-01, -9.52573657e-01, -9.36358988e-01,\n",
       "       -9.17852819e-01, -8.97100568e-01, -8.74152958e-01, -8.49066079e-01,\n",
       "       -8.21901441e-01, -7.92725503e-01, -7.61609554e-01, -7.28629887e-01,\n",
       "       -6.93867087e-01, -6.57406271e-01, -6.19336724e-01, -5.79751551e-01,\n",
       "       -5.38747668e-01, -4.96425271e-01, -4.52888131e-01, -4.08242643e-01,\n",
       "       -3.62598151e-01, -3.16066295e-01, -2.68760979e-01, -2.20797971e-01,\n",
       "       -1.72294617e-01, -1.23369634e-01, -7.41427541e-02, -2.47344282e-02,\n",
       "        2.47344282e-02,  7.41427541e-02,  1.23369634e-01,  1.72294617e-01,\n",
       "        2.20797971e-01,  2.68760979e-01,  3.16066295e-01,  3.62598151e-01,\n",
       "        4.08242643e-01,  4.52888131e-01,  4.96425271e-01,  5.38747668e-01,\n",
       "        5.79751551e-01,  6.19336724e-01,  6.57406271e-01,  6.93867087e-01,\n",
       "        7.28629887e-01,  7.61609554e-01,  7.92725503e-01,  8.21901441e-01,\n",
       "        8.49066079e-01,  8.74152958e-01,  8.97100568e-01,  9.17852819e-01,\n",
       "        9.36358988e-01,  9.52573657e-01,  9.66457307e-01,  9.77975845e-01,\n",
       "        9.87101078e-01,  9.93810713e-01,  9.98088360e-01,  9.99923527e-01,\n",
       "        9.99311686e-01,  9.96254325e-01,  9.90759015e-01,  9.82839167e-01,\n",
       "        9.72514093e-01,  9.59809184e-01,  9.44755375e-01,  9.27389622e-01,\n",
       "        9.07754481e-01,  8.85897756e-01,  8.61873090e-01,  8.35739434e-01,\n",
       "        8.07560444e-01,  7.77405381e-01,  7.45347679e-01,  7.11466193e-01,\n",
       "        6.75843477e-01,  6.38566852e-01,  5.99727690e-01,  5.59420705e-01,\n",
       "        5.17744958e-01,  4.74801928e-01,  4.30697232e-01,  3.85538340e-01,\n",
       "        3.39435965e-01,  2.92503148e-01,  2.44854316e-01,  1.96606517e-01,\n",
       "        1.47877350e-01,  9.87865329e-02,  4.94537354e-02, -8.74227766e-08],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_as_float_or_int16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 11:36:56.287357: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'enable_v2_behavior'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb Cell 1\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Import TF 2.X and make sure we're running eager.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m tf\u001b[39m.\u001b[39;49menable_v2_behavior()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m tf\u001b[39m.\u001b[39mexecuting_eagerly()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/emile/dev/speech-automatic-personality-recognition/tensorflow.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow_hub\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mhub\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'enable_v2_behavior'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Load the module and run inference.\n",
    "module = hub.load('https://tfhub.dev/google/nonsemantic-speech-benchmark/trill-distilled/2')\n",
    "# `wav_as_float_or_int16` can be a numpy array or tf.Tensor of float type or\n",
    "# int16. The sample rate must be 16kHz. Resample to this sample rate, if\n",
    "# necessary.\n",
    "wav_as_float_or_int16 = np.sin(np.linspace(-np.pi, np.pi, 128), dtype=np.float32)\n",
    "emb = module(samples=wav_as_float_or_int16, sample_rate=16000)['embedding']\n",
    "# `emb` is a [time, feature_dim] Tensor.\n",
    "emb.shape.assert_is_compatible_with([None, 2048])\n",
    "\n",
    "print(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
